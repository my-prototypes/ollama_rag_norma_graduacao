# ðŸ§© Ollama RAG Starter Kit (macOS)

Este kit cria um protÃ³tipo de **IA generativa local** com **RAG sobre PDFs** usando **Ollama** + **ChromaDB**.

## âœ… Requisitos
- macOS (Apple Silicon)
- Python 3.10+
- Homebrew
- [Ollama](https://ollama.com) instalado e rodando localmente

## ðŸš€ Passo a passo (rÃ¡pido)
1) Instale o Ollama e inicie o serviÃ§o:
```bash
brew install ollama
ollama serve &
```

2) Baixe um modelo de chat e um de embeddings (recomendado):
```bash
# ollama pull qwen2.5:7b-instruct
ollama pull llama3.2:3b
ollama pull nomic-embed-text
# alternativos:
# ollama pull llama3:8b-instruct
# ollama pull snowflake-arctic-embed:latest
```

3) Crie um ambiente Python e instale dependÃªncias:
```bash
python3 -m venv .venv && source .venv/bin/activate
pip3 install -r requirements.txt
```

4) Coloque seus PDFs na pasta:
```
data/pdfs/
```

5) IngestÃ£o (gera base vetorial persistente em `vectordb/`):
```bash
python3 scripts/ingest_pdfs.py --pdf-dir data/pdfs --collection docs
```

6) Chat com RAG:
```bash
python3 scripts/chat_rag.py --collection docs --model llama3.2:3b --embed nomic-embed-text
```

## ðŸ§  Notas de uso
- **Hardware**: seu Mac mini (16 GB RAM) roda bem modelos atÃ© ~7â€“9B quantizados; usar Ollama simplifica muito.
- **Contexto**: por padrÃ£o pedimos top-5 trechos dos PDFs.
- **PersistÃªncia**: ChromaDB fica em `vectordb/`, pode ser apagado e recriado quando quiser.

## ðŸ”§ VariÃ¡veis de ambiente
Copie `.env.example` para `.env` e ajuste se necessÃ¡rio:
```
cp .env.example .env
```
- `OLLAMA_BASE_URL` (default: http://localhost:11434)

## ðŸ§ª Dicas
- PDFs muito grandes: faÃ§a upload aos poucos e reingira, ou aumente `--chunk-size`/`--chunk-overlap` conforme a necessidade.
- Se quiser mais velocidade, teste modelos menores (ex.: `phi3:mini`).
- Para citaÃ§Ãµes, o script mostra a origem (arquivo e Ã­ndice do trecho).
- DocumentaÃ§Ã£o da API Ollama: https://github.com/ollama/ollama/blob/main/docs/api.md
---
